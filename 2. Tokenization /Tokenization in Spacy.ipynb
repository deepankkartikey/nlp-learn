{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f37623d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (1.23.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (8.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.9.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/deepankkartikey/opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf8070f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8282142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "ottawa\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Let's go to ottawa.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27744a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "strange\n",
      "visited\n",
      "two\n",
      "cities\n",
      ",\n",
      "first\n",
      "delhi\n",
      ",\n",
      "and\n",
      "then\n",
      "mumbai\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. strange visited two cities, first delhi, and then mumbai.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b489afa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "812e8567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f05b099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10a9af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91e3e1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ecc7527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7d21a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eeffc626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].like_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270b4f9",
   "metadata": {},
   "source": [
    "## Extract emails from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82e53b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e7c7b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n==================================================\\n\\nName\\tbirth day   \\temail\\n-----\\t------------\\t------\\nVirat   5 June, 1882    virat@kohli.com\\nMaria\\t12 April, 2001  maria@sharapova.com\\nSerena  24 June, 1998   serena@williams.com \\nJoe      1 May, 1997    joe@root.com\\n\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab519a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4861499",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecff0740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "613dd22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कैसे\n",
      "हो\n",
      "भ्ऐ\n",
      ",\n",
      "मेइन\n",
      "सोने\n",
      "ज\n",
      "रह\n",
      "हुन्\n"
     ]
    }
   ],
   "source": [
    "nlpHindi = spacy.blank(\"hi\")\n",
    "doc = nlpHindi(\"कैसे हो भ्ऐ, मेइन सोने ज रह हुन्\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2f2c6",
   "metadata": {},
   "source": [
    "### Customize Tokenizer - adding special rule in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e159c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"gimme double cheese extra large burger\")\n",
    "\n",
    "tokens = [token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58bd6479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gimme, double, cheese, extra, large, burger]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "705fc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "# customization does not allow gimme --> give, me (tokens cant be modified)\n",
    "# customization can allow to break token into multiple tokens\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de1f9e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gim, me, double, cheese, extra, large, burger]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large burger\")\n",
    "\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61e56f",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3e45345",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Dr. strange is a wizard. Hulk is a mutant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8298c8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. strange is a wizard. Hulk is a mutant."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3eda201",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sent)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.10/site-packages/spacy/tokens/doc.pyx:890\u001b[0m, in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd11271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf495d",
   "metadata": {},
   "source": [
    "### Since, a blank pipeline was created there are no components present in the nlp pipeline.\n",
    "### Hince, we need to add a component to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d21d2e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fd7926f7d40>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30f69861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d6c09309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. strange is a wizard.\n",
      "Hulk is a mutant.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. strange is a wizard. Hulk is a mutant.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b183f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
